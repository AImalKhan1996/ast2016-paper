---
title: "Virtual Mutation Data Analysis for the AST 2016 Paper"
output:
  pdf_document:
    keep_tex: true
    highlight: pygments
---

# Installing the Virtual Mutation Package

This code will load in the virtual mutation package from a local directory. I was not able to get it to work with an
installation from Bitbucket because of permission errors. You will need to change the path so that it is correct for
your file system. Note that ultimately, I plan to have this package available through a public GitHub repository. Then,
in this case, it will be possible to use devtools to perform a standard installation.

```{r, echo=TRUE}
options(scipen=10, width=200)
mypath = "/home/gkapfham/working/research/source/analysis/databases/"
devtools::install_local(paste(mypath, "virtualmutationanalysis", sep=""))
```

# Comparing "Time-Constrained" Original and Virtual Mutation

## Initialise the System

First, load in the libraries that are used in addition to those with the virtualmutationanalysis package (i.e., load all
of the packages not used by virtualmutationanalysis but still used in this RMarkdown file). Note that right now the
virtualmutationanalysis package will automatically load all of the packages that it needs to performs its various
analyses. Now, we are ready to call the functions from the virtualmutationanalysis package and produce the appropriate
summary tables and graphs.

In addition, note that this document uses the term "Original" and the paper itself now uses the term "Standard". As time
permits, this small mistake in the naming convention should be updated.

```{r, echo=TRUE}
library(virtualmutationanalysis)
suppressPackageStartupMessages(library(knitr))
```

## Show the Schemas for the AST 2016 Paper

```{r, echo=TRUE}
time_constrained_data <- read_time_constrained_mutation_data_subset()
time_constrained_data_tidy = time_constrained_data %>% transform_time_constrained_data_for_scores()
viable_schemas <- time_constrained_data_tidy %>% select(schema) %>% unique()
knitr::kable(viable_schemas, format = "latex", booktabs=TRUE)
```

## Visualise the Mutation Scores

Now, plot the graph that shows the mutation scores for the virtual mutation technique and the original mutation method
that can run for the same amount of time as the virtual one.

```{r, fig.width=5.7, fig.height=3.5, echo=TRUE}
visualise_mutation_score_time_constrained(time_constrained_data_tidy)
```

Analysis of the results in this graph:

- Much more variability for the time-constrained method than the virtual one
- However, the mutation scores are often roughly similar between the two techniques, especially for the HyperSQL and
  SQLite DBMSs
- It is important to note that the time-constrained method yields mutation scores that are higher than those produced by
  the virtual mutation technique (e.g., CoffeeOrders and Employee schemas). And, note that the virtual method
  leads to a score that is the same as the standard one. So, time constrained will over estimate the scores sometimes.
- Greater variability in the Person schema is due to the fact that sometimes the mutation score is zero (i.e., all of
  the mutants are killed) and other times it is one (i.e., none of the mutants are killed)

Additional data points to support this analysis:

```{r, echo=TRUE}
time_constrained_data_tidy_print <- time_constrained_data_tidy %>%
  filter(schema %in% c("Person")) %>%
  filter(dbms %in% c("PostgreSQL")) %>%
  filter(score_type %in% c("Time-Constrained"))
knitr::kable(time_constrained_data_tidy_print, format = "latex", booktabs=TRUE)
```

## Visualise the Total Number of Mutants

```{r, fig.width=5.7, fig.height=3.5, echo=TRUE}
time_constrained_data <- read_time_constrained_mutation_data_subset()
time_constrained_data_totals_tidy <- time_constrained_data %>%
  transform_time_constrained_data_for_totals()
visualise_mutation_totals_time_constrained(time_constrained_data_totals_tidy)
```

Analysis of the results in this graph:

- The values for the time-constrained method are averages across the thirty runs
- There is the greatest disparity in the total number of mutants run for the Postgres DBMS
- Running mutation analysis on the Postgres DBMS only allows 0, 1, 2, or 3 mutants to be run
- For SQLite, it is possible to run all of the mutants for a small number of schemas (e.g., StudentResidence)
- I did several "spot checks" and the values in the graph seem to be correct (e.g., low values for Postgres)

## Evaluating the Correlation Between the Scores

```{r, echo=TRUE}
time_constrained_data <- read_time_constrained_mutation_data()
calculate_mutation_score_correlation(time_constrained_data)
```

Analysis of the results in this output:

- The p-value is very low and thus essentially zero. This rules out the hypothesis that the value of $\tau_b$ is equal to
  zero, which would have indicate that there is no correlation between the mutation scores.
- If the correlation has a value of 1 this means that the scores are in perfect agreement.
- If the correlation has a value of -1 this means that the scores are in perfect disagreement.
- The score of $0.49$ indicates that there is a "modest" agreement between the mutation scores.

# Comparing the Original and Virtual Mutation Techniques

## Create a Data Set Combining Original and Virtual Observations

```{r, echo=TRUE}
original_and_virtual_mutation_data = create_original_and_virtual_data()
```

## Visualise the Mutation Analysis Time for Original and Virtual

```{r, fig.width=5.7, fig.height=3.5, echo=TRUE}
visualise_mutation_time_original_virtual(original_and_virtual_mutation_data)
```

Commentary on the results in this output:

- Note that this data has been "log transformed" using a log-to-the-base-ten transformation
- Transformation is done because the Postgres-Original data is on a different scale than all other data
- This means that you will only see variation for Postgres-Original (without transformation)
- Using the log-transformed data shows the basic trends in the data sets

Analysis of the results in this output:

- The original technique on Postgres is the slowest overall, suggesting one situation in which it is always best to
  deploy a virtual technique.
- The log-transformed data shows that the virtual technique is always faster or comparable to the original one for the
  HyperSQL database, especially for larger schemas like JWhoisServer.
- Virtual is sometimes slower than Original for the SQLite DBMS, owing to the fact that SQLite is in-memory. Still, for
  larger schemas, like JWhoisServer, it is better to use virtual than Original.

## Perform a Statistical Analysis on the Execution Timings of the Virtual and Original Techniques

```{r, echo=TRUE}
analyse_wilcox_rank_sum_test(original_and_virtual_mutation_data)
```

Note that this hypothesis testing approach creates two vectors of timings, one for all of the original timings and one
for all of the virtual timings, across all of the schemas. As the p-value is very small, these results reveal that there
is a statistically significant difference between the timings and thus the virtual technique is faster in a statistically
significant fashion. But, it is also important to discern whether there is a practical significance to these results.

## Determine How Frequently Virtual is Better Than Original

```{r, echo=TRUE}
original_and_virtual_data_summary <- original_and_virtual_mutation_data %>%
  summarise_mutation_analysis_time()
knitr::kable(original_and_virtual_data_summary, digits=3)
```

Analysis of the results in this output:

- Note that this table is useful for writing content that will appear in the abstract and at the end of the
  introduction.
- The original technique is only better for five of the configurations studied.
- Original is only better for the SQLite database management system, which is in-memory and in a separate process.
- Original is only better for small database schemas, when it is expected that virtual will not be as useful.
- Original is judged to be "better" in cases where the results are still within the same order of magnitude.
- Normally, original is only better than virtual (for SQLite, small schema) by 50-100 ms.
- Otherwise, the virtual technique is always better than the original technique.
- So, it is acceptable to use the virtual technique in all cases if you are willing to accept small losses for small
  schemas when you are using the fast SQLite database management system.
- TODO: Get a quick summary percentage out of this table.

## Visualise the Time Savings for Using Virtual Instead of Original, with Regard to Mutants

```{r, fig.width=3, fig.height=3.5, echo=TRUE}
original_mutation_data = read_original_mutation_data_subset()
original_and_virtual_mutation_data_mutant_savings = original_and_virtual_mutation_data %>%
  transform_mutation_time_savings(original_mutation_data)
visualise_savings_and_mutants(original_and_virtual_mutation_data_mutant_savings)
```

Analysis of the results in this output:

- Make sure that the paper gives the equation that was used to calculate the percentage savings.
- Note that I did not screen out the results where there is no savings because virtual is more expensive (Chris does
  this in his code, if I am understanding that code correctly).
- For any number of mutants, there is always a high percentage savings for the Postgres DBMS; for this DBMS the
  percentage increases slightly as the number of mutants increases.
- The HyperSQL DBMS always shows percentage of time saved that is positive.
- But, for HyperSQL, the percentage savings is gradual and it tapers off around 75 percent for the greatest number of
  mutants.
- For SQLite, the percentage savings is negative (meaning that it takes longer to use the virtual technique, as
  mentioned in the previous analysis). But, the absolute increase in time is modest for these cases.
- There are four schemas (check which ones) for which there is a positive percentage savings for SQLite.
- Note that when the schema is large, the percentage savings for SQLite is similar to that for HyperSQL. Again, this
  demonstrates that the virtual technique is always useful when doing mutation testing of large schemas.

## Visualise the Time Savings for Using Virtual Instead of Original, with Regard to Tests

```{r, fig.width=3, fig.height=3.5, echo=TRUE}
original_mutation_data = read_original_mutation_data_subset()
original_and_virtual_mutation_data_mutant_savings = original_and_virtual_mutation_data %>%
  transform_mutation_time_savings(original_mutation_data)
visualise_savings_and_tests(original_and_virtual_mutation_data_mutant_savings)
```

Analysis of the results in this output:

- The trends in this graph are similar to what was given for the previous graph.
- This graph should only be included as space is available, place near the other graph.

## Confirm the Results from the Two Graphs with a Table

```{r, echo=TRUE}
original_mutation_data = read_original_mutation_data_subset()
original_and_virtual_mutation_data_mutant_savings = original_and_virtual_mutation_data %>%
  transform_mutation_time_savings(original_mutation_data)
knitr::kable(original_and_virtual_mutation_data_mutant_savings, format = "latex", booktabs=TRUE, digits=3)
```

Analysis of the results in this output:

- This table supports the checking of the results inside of the graphs, a quick check shows things are okay.
- Note that the very poor -92.9 percent increase for the Iso3166 is for results under 100 ms.
- It is likely the case that these different times would actually be indistinguishable from a user's perspective.
- Look into the recent Harman SSBSE about the transformed effect size metric for a citation for the last point.
- So, even when virtual is the "loser" this would not be noticeable. But, when it is the "winner" this would be
  apparent.

Questions about the results analysis:

- Is there every a case where we want to do statistical analysis or effect size calculations?
- I think that the "transformation" idea from Harman would apply to our analysis of the timing values.
