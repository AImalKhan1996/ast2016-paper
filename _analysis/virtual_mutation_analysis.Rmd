---
title: "Virtual Mutation Data Analysis for the AST 2016 Paper"
output:
  pdf_document:
    keep_tex: true
    highlight: pygments
---

# Installing the Virtual Mutation Package

This code will load in the virtual mutation package from a local directory. I was not able to get it to work with an
installation from Bitbucket because of permission errors. You will need to change the path so that it is correct for
your file system.

```{r, echo=TRUE}
options(scipen=10, width=200)
mypath = "/home/gkapfham/working/research/source/analysis/databases/"
devtools::install_local(paste(mypath, "virtualmutationanalysis", sep=""))
```

# Comparing "Time-Constrained" Original and Virtual Mutation

## Initialise the System

First, load in the libraries that are used in addition to those with the virtualmutationanalysis package. Note that
right now the virtualmutationanalysis package will automatically load all of the packages that it needs to performs its
various analyses.

```{r, echo=TRUE}
library(virtualmutationanalysis)
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(grid))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(scales))
```

## Show the Schemas for the AST 2016 Paper

```{r, echo=TRUE}
time_constrained_data <- read_time_constrained_mutation_data_subset()
time_constrained_data_tidy = time_constrained_data %>% transform_time_constrained_data_for_scores()
viable_schemas <- time_constrained_data_tidy %>% select(schema) %>% unique()
knitr::kable(viable_schemas)
```

## Visualise the Mutation Scores

Now, plot the graph that shows the mutation scores for the virtual mutation technique and the original mutation method
that can run for the same amount of time as the virtual one.

```{r, echo=TRUE}
visualise_mutation_score_time_constrained(time_constrained_data_tidy)
```

Analysis of the results in this graph:

- Much more variability for the time-constrained method than the virtual one
- Greater variability in the Person schema is due to the fact that sometimes the score is zero, other times it is one
- Also, the BankAccount schema sees a lot of variability, with noticeable high and low values

Additional data points to support this analysis:

```{r, echo=TRUE}
time_constrained_data_tidy %>%
  filter(schema %in% c("Person")) %>%
  filter(dbms %in% c("Postgres")) %>%
  filter(score_type %in% c("Time-Constrained")) %>%
  print(n=100)
time_constrained_data_tidy %>%
  filter(schema %in% c("BankAccount")) %>%
  filter(dbms %in% c("HyperSQL")) %>%
  filter(score_type %in% c("Time-Constrained")) %>%
  print(n=100)
```

## Visualise the Total Number of Mutants

```{r, echo=TRUE}
time_constrained_data <- read_time_constrained_mutation_data()
time_constrained_data_totals_tidy <- time_constrained_data %>%
  transform_time_constrained_data_for_totals()
visualise_mutation_totals_time_constrained(time_constrained_data_totals_tidy)
```

Analysis of the results in this graph:

- The values for the Time-Constrained method are averages across the thirty runs
- There is the greatest disparity in the total number of mutants run for the Postgres DBMS
- Running mutation analysis on the Postgres DBMS only allows 0, 1, 2, or 3 mutants to be run
- For SQLite, it is possible to run all of the mutants for a small number of schemas (e.g., StudentResidence)
- I did several "spot checks" and the values in the graph seem to be correct (e.g., low values for Postgres)

## Evaluating the Correlation Between the Scores

```{r, echo=TRUE}
time_constrained_data <- read_time_constrained_mutation_data()
calculate_mutation_score_correlation(time_constrained_data)
```

Analysis of the results in this output:

- The p-value is very low and thus essentially zero. This rules out the hypothesis that the value of $\tau_b$ is equal to
  zero, which would have indicate that there is no correlation between the mutation scores.
- If the correlation has a value of 1 this means that the scores are in perfect agreement.
- If the correlation has a value of -1 this means that the scores are in perfect disagreement.
- The score of $0.49$ indicates that there is a "modest" agreement between the mutation scores.

# Comparing the Original and Virtual Mutation Techniques

## Create a Data Set Combining Original and Virtual Observations

```{r, echo=TRUE}
original_and_virtual_mutation_data = create_original_and_virtual_data()
```

## Visualise the Mutation Analysis Time for Original and Virtual

```{r, echo=TRUE}
visualise_mutation_time_original_virtual(original_and_virtual_mutation_data)
```

Commentary on the results in this output:

- Note that this data has been "log transformed" using a log-to-the-base-ten transformation
- Transformation is done because the Postgres-Original data is on a different scale than all other data
- This means that you will only see variation for Postgres-Original (without transformation)
- Using the log-transformed data shows the basic trends in the data sets

Analysis of the results in this output:

- The original technique on Postgres is the slowest overall, suggesting one situation in which it is always best to
  deploy a virtual technique.
- The log-transformed data shows that the virtual technique is always faster or comparable to the original one for the
  HyperSQL database, especially for larger schemas like JWhoisServer.
- Virtual is sometimes slower than Original for the SQLite DBMS, owing to the fact that SQLite is in-memory. Still, for
  larger schemas, like JWhoisServer, it is better to use virtual than Original.

## Determine How Frequently Virtual is Better Than Original

```{r, echo=TRUE}
original_and_virtual_data_summary <- original_and_virtual_mutation_data %>%
  summarise_mutation_analysis_time()
knitr::kable(original_and_virtual_data_summary, digits = 3)
```

Analysis of the results in this output:

- Note that this table is useful for writing content that will appear in the abstract and at the end of the
  introduction.
- The original technique is only better for five of the configurations studied.
- Original is only better for the SQLite database management system, which is in-memory and in a separate process.
- Original is only better for small database schemas, when it is expected that virtual will not be as useful.
- Original is judged to be "better" in cases where the results are still within the same order of magnitude.
- Normally, original is only better than virtual (for SQLite, small schema) by 50-100 ms.
- Otherwise, the virtual technique is always better than the original technique.
- So, it is acceptable to use the virtual technique in all cases if you are willing to accept small losses for small
  schemas when you are using the fast SQLite database management system.
- TODO: Get a quick summary percentage out of this table.

## Visualise the Time Savings for Using Virtual Instead of Original, with Regard to Mutants

```{r, echo=TRUE}
original_mutation_data = read_original_mutation_data_subset()
original_and_virtual_mutation_data_mutant_savings = original_and_virtual_mutation_data %>%
  transform_mutation_time_savings(original_mutation_data)
visualise_savings_and_mutants(original_and_virtual_mutation_data_mutant_savings)
```

Analysis of the results in this output:

## Visualise the Time Savings for Using Virtual Instead of Original, with Regard to Tests

```{r, echo=TRUE}
original_mutation_data = read_original_mutation_data_subset()
original_and_virtual_mutation_data_mutant_savings = original_and_virtual_mutation_data %>%
  transform_mutation_time_savings(original_mutation_data)
visualise_savings_and_tests(original_and_virtual_mutation_data_mutant_savings)
```
Analysis of the results in this output:

## Confirm the Results from the Two Graphs with a Table

```{r, echo=TRUE}
original_mutation_data = read_original_mutation_data_subset()
original_and_virtual_mutation_data_mutant_savings = original_and_virtual_mutation_data %>%
  transform_mutation_time_savings(original_mutation_data)
knitr::kable(original_and_virtual_mutation_data_mutant_savings, digits = 3)
```

Analysis of the results in this output:
