%!TEX root=../ast2016.tex

\vspace{-1em}
\section{Experimental Setup}
\label{sec:experimental-setup}

To evaluate our \vma technique, we investigate three research questions, comparing it against the \Original approach to mutation analysis for relational database schemas, which uses an instance of a DBMS (and is hereon simply referred to as the ``\Original'' approach):

\vspace{5pt}

\noindent
\textbf{RQ1 (Efficiency):} How does the time overhead of \vma compare to the \Standard technique's cost, and how does
this vary depending on the DBMS in use?

\vspace{5pt}

\noindent
\textbf{RQ2 (Scalability):} How does the time savings from using \vma scale when increasing either the number
of analysed mutants or the number of executed tests?

\vspace{5pt}

\noindent
\textbf{RQ3 (Effectiveness):} How does the mutation score of \vma compare to the score of a time-constrained method that
is only permitted to run for as long as the virtual one?

\begin{sloppypar}
\inlineheading{Methodology}
% Explain the number of trials and the basics about test data generation
To answer the first and second research \mbox{questions}, we recorded the time need to run the \Original approach and \vma 30 times each, for each of the schemas listed in Table~\ref{tbl:study-schemas} and with each of the three DBMSs---\HyperSQL, \Postgres and \SQLite. 
%
%% On the subject schemas:
The subject schemas we use appear in open source projects (JWhoisServer and MozillaPermissions), SQL conformance suites and DBMS sample schemas (NistWeather and Iso3166 respectively), while the remainder appear in textbooks, laboratory assignments and online tutorials, where they were provided as examples. 
% TODO: add a table of numbers of mutants?
% TODO: explain we removed ineffective mutants
% TODO: should we explain why we used the DBMSs we did?
\end{sloppypar}

For each run, we used a test suite that was automatically generated by a search-based method; the test generator was seeded with a unique random value.

% Give more details about the test data generation procedure

Details of the specific test suite generation algorithms used are given by McMinn \etal~\cite{McMinn2015}. We used the \AVM technique since past experiments have show it to be the most reliable automated method for generating test suites that achieve high levels of test coverage~\cite{McMinn2015}. The coverage criterion we used was a combination of ``ClauseAICC'', ``AUCC'' and ``ANCC'', thus merging the strongest criteria for testing the integrity constraints of database schemas.

% Explain the third research question's process

To answer the third research question, we ran the \Original technique 30 times, performing a mutation analysis that randomly selected mutants until the time taken for the corresponding run of the 30 repetitions of \vma was exhausted. In other words, the \Original method was run in a ``time-constrained'' fashion where its maximum allotted time was equal to the time needed to complete the comparable \vma.

% State the execution environment and the tools used for the experiments

We performed all of the experiments with the \SA tool \cite{Kapfhammer2013,McMinn2015,Wright2014}, compiled with the Java Development Kit 7 compiler and executed with the Linux version of the 64-bit Oracle Java 1.7 virtual machine.  Experiments were executed on an Ubuntu 14.04 workstation, with a 3.13.0-44 Linux 64-bit kernel, a quad-core 2.4GHz CPU and 12GB RAM. All input (i.e., the database schemas) and output (i.e., the result files) were stored on the workstation's local disk. We used the default configuration of \PostgreSQL version 9.3.5, \HyperSQL version 2.2.8 and \SQLite 3.8.2.  \HyperSQL and \SQLite were used with ``in-memory'' mode enabled.

\input{tables/schemas.tex}

\inlineheading{Analysis Methods}
% Describe the meaning of the box and whisker plots
Figures~\ref{fig:graphic_bwplot_schema_analysistime_org_vm} and ~\ref{fig:graphic_bwplot_schema_mutationscore_vm_tcm} furnish box and whisker plots.  In these plots the box itself represents the interquartile range (IQR), or the measure of statistical dispersion that is the difference between the first and third quartiles. Moreover, the upper whisker extends from the top of the box to the highest value that is within $1.5$ times the IQR, the lower whisker goes from the bottom of the box to the lowest value within $1.5$ times the IQR, and the thick horizontal line represents the median value.  Additionally, these box plots use a filled circle for an outlier and an open diamond for the mean value.

% Describe the statistical and effect size tests

To statistically analyse the trends in Figure~\ref{fig:graphic_bwplot_schema_analysistime_org_vm} we conducted tests for significance with the nonparametric \wilcoxon, using the sets of 30 execution times obtained with a specific DBMS and the \Original and \vma techniques.  A \pvalue less than $0.05$ is deemed to be significant.  To complement significance tests, the nonparametric \^{A}\textsubscript{12} statistic of Vargha and Delaney \cite{Vargha2000} was used to compute effect sizes, which determine the average probability that one approach ``out performs'' another.  We followed the guidelines of Vargha and Delaney in that an effect size is deemed to be ``large'' if the value of \atwelve~is $< 0.29$ or $> 0.71$, ``medium'' if \atwelve~is $< 0.36$ or $> 0.64$ and ``small'' if \atwelve~is $< 0.44$ or $> 0.56$.  Values of \atwelve~close to the $0.5$ value are deemed to have no effect size.  When discussing effect sizes for the execution times of the two methods, we follow Neumann \etal~\cite{Neumann2015} and say that a value of \atwelve closer to zero indicates that virtual is the preferred technique while a value near one shows that \Original is faster.

% Describe the calculation of the percentage of mean time saved

As the number of mutants subject to analysis and the number of generated tests increases, Figure~\ref{fig:graphic_scatterplot_mutantstests_percentagetimesaved} plots the percentage of mean time saved from using \vma instead of the \Original method.  This value is determined by first calculating the mean execution time from the thirty trials of both the \Standard and the \virtual techniques. If $T_s$ denotes the mean time taken by the standard method and $T_v$ is the mean time needed for the virtual one, then we calculated the percentage of mean time saved by $({T_s - T_v})/{T_s}$.

% Describe the calculation of the correlation coefficient
% TODO: The sentence that starts with "This ..." is not correct yet!

We employed a correlation statistic to determine how the mutation scores of the \tc method correspond to those of \vma.  Due to the possibility of rank ties, which are not supported by a number of correlation measures, we chose to use Kendall's \taub~coefficient, as provided by the ``Kendall'' R package \cite{KendallCran}. This allows for rank ties and provides a measurement of correlation between -1 and 1, representing a strong negative and strong positive association, respectively, with 0 indicating that there is no correlation. Following Inozemtseva and Holmes, we adopt the Guildford scale to describe the correlation values, with the absolute value of a coefficient being described as ``low'' when it is less than $0.4$, ``moderate'' when it is between $0.4$ and $0.7$, ``high'' when ranging from $0.7$ to $0.9$, and ``very high'' when it is greater than \mbox{$0.9$ \cite{Inozemtseva2014}}.

\inlineheading{Threats to Validity}
% TODO: Explain that we check the mutation score to ensure that Standard and Virtual are the same!
% TODO: Reference the SSBSE 2015 paper that talks about transformation of the values; we also
% did this and we are certain that there is no difference (we also looked at different thresholds).