%!TEX root=ast2016.tex

\vspace{-1em}
\section{Experimental Setup}
\label{sec:experimental-setup}

\input{tables/schemas.tex}

To evaluate our \vma technique, we investigate three research questions, comparing it against the \Original approach to mutation analysis for relational database schemas, which uses an actual instance of a DBMS (and is hereon simply referred to as the ``\Original'' approach):

\vspace{5pt}

\noindent
\textbf{RQ1 (Efficiency):} How does the time overhead of \vma compare to the \Standard technique's cost, and how does
this vary depending on the DBMS in use?

\vspace{5pt}

\noindent
\textbf{RQ2 (Scalability):} How does the time savings from using \vma scale when increasing either the number
of analysed mutants or the number of executed tests?

\vspace{5pt}

\noindent
\textbf{RQ3 (Effectiveness):} How does the mutation score of \vma compare to the score of a time-constrained method that
is only permitted to run for as long as the virtual one?

% PURPOSE: Explain the number of trials and the basics about test data generation and describe the subject schemas

\begin{sloppypar}
\inlineheading{Methodology} To answer the first and second research \mbox{questions}, we recorded the time needed to run the \Original approach and \vma 30 times each, for each of the subject schemas listed in Table~\ref{tbl:study-schemas} and with each of the three DBMSs---\HyperSQL, \Postgres and \SQLite.  Two schemas we used appear in open-source projects (i.e., JWhoisServer and MozillaPermissions), while others appear in SQL conformance suites and DBMS sample sets (i.e., NistWeather and Iso3166, respectively). Previous studies have shown that the remaining schemas were challenging to handle for random test generators \cite{McMinn2015} and the open-source DBMonster tool \cite{Kapfhammer2013}.  Between $9$ and $184$ mutants were generated for each schema, as shown by \Table{tbl:num-mutants}, and between $426$ and $449$ in total for the three DBMSs. These total correspond to numbers following the removal of certain ineffective mutants---mutants found to be equivalent to the original or some other mutant, or ``stillborn'' \cite{Wright2014}. Due to differing behaviors and interpretations of the SQL standard, the notion of ``equivalence'' differs from DBMS to DBMS, leading to differing numbers being reported in the table for some schemas.
\end{sloppypar}

% GMK NOTE: Can we drop this phrase? Removing it improves formatting and avoids repeats in the sentence.
% with different DBMSs.

% TODO? we could also try to explain why we used the DBMSs we did

% PURPOSE: Give more details about the test data generation procedure

For each run, we used a test suite that was automatically generated by a search-based method; the test generator was seeded with a unique random value. Details of the specific generation algorithms used are given by McMinn \etal~\cite{McMinn2015}. We used the \AVM technique since past experiments have show it to be the most reliable automated method for generating test suites that achieve high levels of test coverage~\cite{McMinn2015}. The coverage criterion we used was a combination of ``ClauseAICC'', ``AUCC'' and ``ANCC'', thus merging the strongest criteria for testing the integrity constraints of schemas.

% PURPOSE: Explain the third research question's process

To answer the third research question, we ran the \Original technique $30$ times, performing a mutation analysis that randomly selected mutants until the time taken for the corresponding run of the $30$ repetitions of \vma was exhausted. In other words, the \Original method was run in a ``time-constrained'' fashion where its maximum allotted time was equal to the time needed to complete the comparable \vma.

% PURPOSE: State the execution environment and the tools used for the experiments

We performed all of the experiments with our \SA tool \cite{Kapfhammer2013,McMinn2015,Wright2014}, compiled with the Java Development Kit 7 compiler and executed with the Linux version of the 64-bit Oracle Java 1.7 virtual machine.  Experiments were executed on an Ubuntu 14.04 workstation, with a 3.13.0-44 Linux 64-bit kernel, a quad-core 2.4GHz CPU and 12GB RAM. All input (i.e., the database schemas) and output (i.e., the result files) were stored on the workstation's local disk. We used the default configuration of \PostgreSQL version 9.3.5, \HyperSQL version 2.2.8 and \SQLite 3.8.2.  \HyperSQL and \SQLite were used with ``in-memory'' mode enabled.

\input{tables/num-mutants.tex}

% PURPOSE: Describe the meaning of the box and whisker plots

\inlineheading{Analysis Methods} Figures~\ref{fig:graphic_bwplot_schema_analysistime_org_vm} and ~\ref{fig:graphic_bwplot_schema_mutationscore_vm_tcm} furnish box and \mbox{whisker plots}.  In these plots the box itself represents the interquartile range (IQR), or the measure of statistical dispersion that is the difference between the first and third quartiles. Moreover, the upper whisker extends from the top of the box to the highest value that is within $1.5$ times the IQR, the lower whisker goes from the bottom of the box to the lowest value within $1.5$ times the IQR, and the thick horizontal line represents the median value. Also, these box plots use a filled circle for an outlier and an open diamond for the mean value.

% PURPOSE: Describe the statistical and effect size tests

To statistically analyse the trends in Figure~\ref{fig:graphic_bwplot_schema_analysistime_org_vm} we conducted tests for significance with the nonparametric \wilcoxon, using the sets of 30 execution times obtained with a specific DBMS and the \Original and \vma techniques.  A \pvalue less than $0.05$ is deemed to be significant.  To complement significance tests, the nonparametric \^{A}\textsubscript{12} statistic of Vargha and Delaney \cite{Vargha2000} was used to compute effect sizes, which determine the average probability that one approach ``out performs'' another.  We followed the guidelines of Vargha and Delaney in that an effect size is deemed to be ``large'' if the value of \atwelve~is $< 0.29$ or $> 0.71$, ``medium'' if \atwelve~is $< 0.36$ or $> 0.64$ and ``small'' if \atwelve~is $< 0.44$ or $> 0.56$.  Values of \atwelve~close to the $0.5$ value are deemed to have no effect size.  When discussing effect sizes for the execution times of the two methods, we follow Neumann \etal~\cite{Neumann2015} and say that a value of \atwelve closer to zero indicates that virtual is the preferred technique while a value near one shows that \Original is faster.

% PURPOSE: Describe the calculation of the percentage of mean time saved

As the number of mutants subject to analysis and the number of generated tests increases, Figure~\ref{fig:graphic_scatterplot_mutantstests_percentagetimesaved} plots the percentage of mean time saved from using \vma instead of the \Original method.  This value is determined by first calculating the mean execution time from the thirty trials of both the \Standard and the \virtual techniques. If $T_s$ denotes the mean time taken by the standard method and $T_v$ is the mean time needed for the virtual one, then we calculated the percentage of mean time saved by $({T_s - T_v})/{T_s}$.

% PURPOSE: Describe the calculation of the correlation coefficient

We employed a correlation statistic to determine how the mutation scores of the \tc method correspond to those of \vma.  Due to the possibility of rank ties, which are not supported by a number of correlation measures, we chose to use the tie-aware Kendall's \taub~coefficient, as provided by the ``Kendall'' R package \cite{KendallCran}.  Kendall's \taub~provides a measurement of correlation between -1 and 1, representing a strong negative and strong positive association, respectively, with 0 indicating that there is no correlation. Following Inozemtseva and Holmes, we adopt the Guildford scale to describe the correlation values, with the absolute value of a coefficient being described as ``low'' when it is less than $0.4$, ``moderate'' when it is between $0.4$ and $0.7$, ``high'' when ranging from $0.7$ to $0.9$, and ``very high'' when it is greater than \mbox{$0.9$ \cite{Inozemtseva2014}}.

% These are the threats to validity:

% -- Randomness in the SA tool and the generated test suite
% -- Randomness in the selection of mutants for time-constrained
% -- Variability in the running time of the mutation analysis
% ---> So, ran multiple trials and did statistical analysis
% ---> When calculated effect sizes, we also looked at thresholds

\inlineheading{Threats to Validity} There are several threats to the validity of the empirical results presented in this paper. First, several of the techniques that we evaluated (e.g., the test data generator in \SA and the mutant selection by the \tcm~method) employ randomness. Additionally, background processes running during experimentation may introduce small random differences in the timings for the mutation analysis methods. To control for these threats we ran 30 trials and used box and whisker plots and statistical tests to analyse the results. Following the advice of Neumann \etal~\cite{Neumann2015}, we disregarded all timings below $100$ ms --- as they would not be perceived by users of our tool --- to ensure that we did not misapply the Vargha-Delaney effect size.

% -- Defects in the test data generator
% -- Defects in the virtual mutation analysis
% ---> So, we tested these programs and checked the mutation scores

% -- Defects in the analysis routines
% --> So, we tested this these programs

% -- Limited number of subjects in the empirical study
% --> But, these subjects are representative and open-source


