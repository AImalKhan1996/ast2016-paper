%!TEX root=../ast2016.tex

\section{Background}
\label{sec:background}

\input{figures/nist-weather}

\inlineheading{Relational Database Schemas}
\Figure{fig:nistweather} shows SQL \sql{CREATE TABLE} statements for the ``\NistWeather'' schema, which is a part of the NIST SQL conformance test suite\footnote{\url{http://www.itl.nist.gov/div897/ctg/sql_form.htm}}. The schema defines two tables. The ``\sql{Stats}'' table (lines 10--19) is for storing rainfall and temperature statistics for a given month pertaining to a particular weather station, the details of which are stored in the ``\sql{Station}'' table (lines 1--9). The definition of each table involves a number of different columns, each with an associated data type, and a series of integrity constraints, which are highlighted in the figure. 

Defining integrity constraints protects the validity and consistency of data stored in the tables of the database. For instance, the \sql{MONTH} columns of the \sql{Stats} table has a ``\CHECK'' constraint defined on it (line 13) that ensures an integer \sql{MONTH} value can only be between 1 and 12. Further \CCs defined on both tables ensure that other column values are within certain valid ranges (lines 6, 8, 15 and 17). Relational databases allow columns to have missing or unknown values (denoted by the ``\NULL'' marker). To prevent inconsistency (for instance, with the \sql{MONTH} column) several columns have a ``\NOTNULL'' constraint defined on them, enforcing values to be present for those columns in all rows of the table concerned.
Furthermore, the \sql{Stats} table involves a ``foreign key'', defined on line 11. Here, ``\sql{ID}'' column values in the \sql{Stats} table must match some value 
for the \sql{ID} column in a row of the \sql{Station} table. Finally, both tables have ``primary keys'' defined on them (lines 2 and 18). A primary key specifies a set of columns in the table that must have distinct sets of values for each row, and ensures the row is uniquely identifiable. 
%% This sentence could be cut if there is little space:
The primary key of the \sql{Stats} is multicolumn, involving the \sql{ID} and \sql{MONTH} columns.

\inlineheading{Testing Integrity Constraints}
Relational database schemas are an important artifact in a software application, and integrity constraints are a key part of their definition. Poorly or incorrectly specified integrity constraints for a schema may leave an application open to a range of serious failures --- for example, non-unique login IDs or negative values for prices or stock levels. For this reason, testing the integrity constraints of a database schema is an important activity that is recommended by industry practitioners \cite{DzoneDatabaseTesting}.

In our previous work, we sought to define coverage criteria for the integrity constraints of a database schema \cite{McMinn2015}. These coverage criteria mandate the creation of test cases with the aim of demonstrating that the schema has been correctly specified for the purpose of admitting valid values into the database, while also rejecting invalid ones. Each test case is designed to exercise a specific integrity constraint defined for the schema, causing it to either be a) satisfied, by submitting rows of database values that are valid; or, b) violated, by submitting rows of values that are invalid. 

In practice, each test case takes the form of a series of SQL \INSERT statements that encode the rows of values with which the schema will be tested. For example, the following \INSERT statement could be used to check that the integrity constraints of the ``\sql{Station}'' table admits certain values as expected. Given an empty table, the values embodied in the statement should be correctly inserted into the table:

\vspace{-.25em}
\begin{center}
\scalebox{\inlinescalefactor}{
\begin{tabular}{l}
\sql{INSERT INTO Station(ID, CITY, STATE, LAT\_N, LONG\_W)} \\ 
\sql{VALUES(1, 'Austin', 'TX', 30, 98);}
\end{tabular}}
\end{center}
\vspace{-.25em}

Further \INSERT statements could then be used to test that the table {\it rejects} certain values as expected, for example using \NULL or out of range values for the \sql{LAT\_N} or \sql{LONG\_W} columns, or by attempting to use a value for \sql{ID} that has already been inserted into the table. Of course, each of these \INSERT statements will be rejected, due to violation of the table's \NOTNULL, \CHECK and \PKCs respectively. 

Whereas traditional program testing involves assertions over values outputted from a program, database schema testing involves checking that \INSERT statements were accepted or rejected as expected. If the acceptance-rejection pattern for a series of \INSERT statements differs from that which was expected, a specification error may exist in the definition of the schema.

\inlineheading{Mutation Analysis of Relational Database Schemas}
Once a test suite has been created, its strength --- that is, its potential fault-finding capability --- can be evaluated using mutation analysis \cite{Jia2011}. Mutation analysis involves seeding small faults into the artifact under test to create {\it mutants} and then checking to find if the test suite behaves differently with the mutant compared with the original artifact. If a difference in behavior is found, the test suite is capable of distinguishing the faulty artifact from the original.

For instance, seeding a fault into the NistWeather schema could take the form of removing the \NOTNULL constraint on the \sql{MONTH} column of the \sql{Stats} table. In the faulty version, \NULL values would be admitted into a database table for the \sql{MONTH} column that would have previously have been rejected, due to violation of the integrity constraint. Therefore, an \INSERT statement with otherwise valid values for the \sql{Stats} table but for \NULL for the \sql{MONTH} column would be {\it accepted} with the mutant schema, but {\it rejected} for the original schema. A test suite with a test case involving such an \INSERT statement is therefore capable of detecting such a fault, and the mutant is said to be ``killed''. However, if the test suite did not involve any \INSERT statements with \NULL for \sql{MONTH}, the difference between the mutant and the original schema would not be exposed, and the mutant would still be alive. Such a test suite would have a lower {\it mutation score} --- the number of mutants killed divided by the total number of mutants --- and would therefore be regarded as having a weaker fault detection capability. 

Previous work \cite{Kapfhammer2013,Wright2013,Wright2014} has developed a series of mutation operators for the integrity constraints of relational database schemas. These include operators that add, remove and replace columns from constraints defined over one or more columns (e.g., \PK and \FKCs), add and remove \NNCs for columns, remove \CCs and alter the relational operators used within them (e.g., substituting ``\sql{>}'' for ``\sql{>=}''). 

\inlineheading{Execution Cost of Mutation Analysis}
A longstanding problem with mutation analysis is the issue of its high execution cost. The test suite needs to be executed with each mutant, and mutation analysis tends to produce large numbers of mutants due to the large number of potential ways in which faults can be seeded into software artefacts, such as programs and relational database schemas. This 
problem
%% Not mentioning scalability any more.
%potential lack of scalability of mutation analysis for large software systems 
has prompted several researchers to develop techniques to reduce its execution costs. These were categorized into three groups by Offutt and Untch \cite{Offutt2001}: ``do fewer'', ``do smarter'' and ``do faster''. As its name suggests, the ``do fewer'' category involves evaluating a subset of the complete set of mutants according to some selection strategy, while the latter two categories involve techniques that approximate the result of standard mutation analysis, or otherwise reduce its execution cost.  

Like mutation analysis for programs, database schema mutation analysis is a time-consuming process. With database schema mutation analysis, one significant addition to the time cost is the overhead of communicating with a DBMS over a network socket. This can be a bottleneck even when the DBMS is running on the same machine as the process conducting the analysis and submitting the relevant SQL commands. In previous work, we sought to minimize the amount of communication that needed to take place, for example by combining all mutants into a single database schema \cite{Wright2013}. This however, does not completely eliminate communication costs. In this paper, we present a technique that reduces its execution costs, by evaluating mutants {\it virtually}. 

% accurately computes the results of standard mutation analysis

