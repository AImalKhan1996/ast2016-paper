%!TEX root=ast2016.tex

% \section{Empirical Study}
% \label{sec:empirical-study}

\section{Experimental Setup}
\label{sec:experimental-setup}

% TODO: Add in one introductory sentence to start off this section

\subsection{Relational Database Schemas}
\label{sec:subjects}

\input{tables/schemas.tex}

\begin{itemize}
  \item 32 schemas
    \begin{itemize}
      \item 1 to 42 tables
      \item 3 to 309 columns
      \item 0 to 134 constraints
      \item Includes each of the types of constraint supported by \SchemaAnalyst (\PK, \FK, \NOTNULL, \UNIQUE and \CHECK constraints)
    \end{itemize}
  \item 3 DBMSs -- \Postgres, \HyperSQL (in-memory), \SQLite (in-memory)
  \item 30 repeated trials
  \item 2 techniques -- \Original and \VirtualMutationAnalysis
  \item Metrics -- time taken for mutation analysis, number of mutants, number of test cases
  \item Mention that the mutation scores of the Original technique are used to validate the results given by virtual mutation analysis, and that in every case the results were identical. (Possibly repeat this in the threats section?)
\end{itemize}

\subsection{Research Questions}
\label{sec:research-questions}

The experiments in this paper investigate three research questions, which we state in the following fashion.

\vspace{5pt}

\noindent
\textbf{RQ1 (Efficiency):} How does the time overhead of \vma compare to the \Standard technique's cost, and how does
this vary depending on the DBMS in use?

\vspace{5pt}

\noindent
\textbf{RQ2 (Scalability):} How does the time savings from using \vma scale when increasing either the number
of analysed mutants or the number of executed tests?

\vspace{5pt}

\noindent
\textbf{RQ3 (Effectiveness):} How does the mutation score of \vma compare to the score of a time-constrained method that
is only permitted to run for as long as the virtual one?

\subsection{Methodology}
\label{sec:methodology}

% Explain the number of trials and the basics about test data generation

To answer the first and second research questions, we ran the \Original technique and \vma 30 times each, for every one of the schemas in Table~\ref{tbl:study-schemas} and with each of the three DBMSs, recording the time taken. For each run, we used a test suite that was automatically generated by a search-based method; the test generator was seeded with a unique random value.

% Give more details about the test data generation procedure

Details of the specific test suite generation algorithms used are given by McMinn \etal~\cite{McMinn2015}. We used the \AVM technique since past experiments have show it to be the most reliable automated method for generating test suites that achieve high levels of test coverage~\cite{McMinn2015}. The coverage criterion we used was a combination of ``ClauseAICC'', ``AUCC'' and ``ANCC'', thus merging the strongest criteria for testing the integrity constraints of database schemas.

% Explain the third research question's process

To answer the third research question, we ran the \Original technique 30 times, performing a mutation analysis that randomly selected mutants until the time taken for the corresponding run of the 30 repetitions of \vma was exhausted. In other words, the \Original method was run in a ``time-constrained'' fashion where its maximum allotted time was equal to the time needed to complete the comparable \vma.

% State the execution environment and the tools used for the experiments

We performed all of the experiments with the \SA tool \cite{Kapfhammer2013,McMinn2015,Wright2014}, compiled with the Java Development Kit 7 compiler and executed with the Linux version of the 64-bit Oracle Java 1.7 virtual machine.  Experiments were executed on an Ubuntu 14.04 workstation, with a 3.13.0-44 Linux 64-bit kernel, a quad-core 2.4GHz CPU and 12GB RAM. All input (i.e., the database schemas) and output (i.e., the result files) were stored on the workstation's local disk. We used the default configuration of \PostgreSQL version 9.3.5, \HyperSQL version 2.2.8 and \SQLite 3.8.2.  \HyperSQL and \SQLite were used with ``in-memory'' mode enabled.

\subsection{Analysis Methods}
\label{sec:analysis-methods}

% Describe the meaning of the box and whisker plots

Figures~\ref{fig:graphic_bwplot_schema_analysistime_org_vm} and ~\ref{fig:graphic_bwplot_schema_mutationscore_vm_tcm} furnish box and whisker plots.  In these plots the box itself represents the interquartile range (IQR), or the measure of statistical dispersion that is the difference between the first and third quartiles. Moreover, the upper whisker extends from the top of the box to the highest value that is within $1.5$ times the IQR, the lower whisker goes from the bottom of the box to the lowest value within $1.5$ times the IQR, and the thick horizontal line represents the median value.  Additionally, these box plots use a filled circle for an outlier and an open diamond for the mean value.

% Describe the statistical and effect size tests

To statistically analyse the trends in Figure~\ref{fig:graphic_bwplot_schema_analysistime_org_vm} we conducted tests for significance with the nonparametric Wilcoxon rank-sum test, using the sets of 30 execution times obtained with a specific DBMS and the \Original and \vma techniques.  A {\it p}-value of less than $0.05$ is deemed to be significant.  To complement significance tests, the nonparametric \^{A}\textsubscript{12} statistic of Vargha and Delaney \cite{Vargha2000} was used to compute effect sizes, which determine the average probability that one approach out performs another.  We followed the guidelines of Vargha and Delaney in that an effect size is deemed to be ``large'' if the value of \atwelve~is $< 0.29$ or $> 0.71$, ``medium'' if \atwelve~is $< 0.36$ or $> 0.64$ and ``small'' if \atwelve~is $< 0.44$ or $> 0.56$.  Values of \atwelve~close to the $0.5$ value are deemed to have no effect size.

% Describe the calculation of the percentage of mean time saved and the correlation coefficient

\subsection{Threats to Validity}
\label{sec:threats-to-validity}

% TODO: Explain that we check the mutation score to ensure that Standard and Virtual are the same!
% TODO: Reference the SSBSE 2015 paper that talks about transformation of the values; we also
% did this and we are certain that there is no difference (we also looked at different thresholds).

\section{Empirical Results}
\label{sec:empirical-results}

% PURPOSE: Compare the virtual mutation technique to the original one in terms of their execution time, showing that
% virtual mutation is often significantly faster than the standard approach to mutation analysis.

\subsection{Comparing Original and Virtual Mutation}
\label{sec:empirical-study-RQ-original-virtual-time}
\input{sections/empirical-study-RQ-original-virtual-time}

% PURPOSE: Investigate the scalability of virtual mutation, looking at how the percentage of mean time saved varies as
% the number of mutants and the number of tests increases, revealing the best configurations for this method.

\subsection{Scalability of Virtual Mutation}
\label{sec:empirical-study-RQ-scalability-mutants-tests}
\input{sections/empirical-study-RQ-scalability-mutants-tests}

% PURPOSE: Compare the virtual and time-constrained techniques in terms of their mutation score and the number of
% mutants that are actually executed during mutation analysis, showing the superiority of virtual mutation.

\subsection{Virtual and Time-Constrained Mutation}
\label{sec:empirical-study-RQ-virtual-time-constrained-virtual}
\input{sections/empirical-study-RQ-time-constrained-virtual}

